#MyCrawler.py
import configparser
import re
import requests
from urllib.parse import urlparse, urljoin
from pymongo import MongoClient
import certifi
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from datetime import datetime
from tkinter import Tk, simpledialog
from urllib.robotparser import RobotFileParser
import configparser

INI_URL = "https://get-infomation.net/list_collection/latest_setting.ini"  # â† å®Ÿéš›ã®URLã«å¤‰æ›´ã—ã¦ãã ã•ã„
EXE_URL = "https://get-infomation.net/list_collection/MyCrawler.exe"
LOCAL_INI_PATH = "setting.ini"
EXE_PATH = "MyCrawler.exe"

# å®šæ•°
MAX_NEW_URLS_PER_OWNER = 10
MAX_TOTAL_URLS_PER_DAY = int(config.get("CRAWLER", "max_urls_per_day", fallback="100"))
maxCountPerDay = 0

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")
username = config["USER"]["id"]

# MongoDBæ¥ç¶š
MONGO_URI = config["USER"]["mongo_uri"]
client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())
db = client["form_database"]
forms_collection = db["forms"]
keywords_collection = db["keywords"]
urls_collection = db["urls"]


# .iniã¿è¾¼ã¿
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")
username = config["USER"]["id"]
# ä¼æ¥­åã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã«è¨˜éŒ²ã—ã¦ã‚¹ã‚­ãƒƒãƒ—åˆ¤æ–­
processed_domains = {}

def is_same_company(domain, company_name):
    if not company_name:
        return False  # ç©ºæ¬„ä¼æ¥­åã¯å¸¸ã«é€šã™
    if domain not in processed_domains:
        processed_domains[domain] = set()
    if company_name in processed_domains[domain]:
        return True
    processed_domains[domain].add(company_name)
    return False

def is_allowed_by_robots(url, user_agent='*'):
    try:
        robots_url = urljoin(url, "/robots.txt")
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception as e:
        send_log_to_server(f"âš ï¸ robots.txt ã®ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return True  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è¨±å¯æ‰±ã„

def send_log_to_server(message):
    import configparser
    config = configparser.ConfigParser()
    config.read("setting.ini", encoding="utf-8")
    user = config["USER"].get("id", "unknown")

    print(message)
    try:
        res = requests.get("https://form-dashboard.onrender.com/log", params={"msg": message, "user": user}, timeout=5)
        if res.status_code == 200:
            print("[SERVER] ãƒ­ã‚°é€ä¿¡æˆåŠŸ")
    except Exception as e:
        print(f"[ERROR] ãƒ­ã‚°é€ä¿¡å¤±æ•—: {e}")


# USERã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¢ºèª
if "USER" not in config:
    config["USER"] = {}

# passãƒã‚§ãƒƒã‚¯
if not config["USER"].get("pass"):
    # GUIã§å…¥åŠ›ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰Tkç„¡åŠ¹åŒ–ï¼‰
    root = Tk()
    root.withdraw()
    pw = simpledialog.askstring("åˆå›ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¨­å®š", "ãƒ­ã‚°ã‚¤ãƒ³ç”¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªå‹•çš„ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼‰")
    root.destroy()

    if pw:
        config["USER"]["pass"] = pw
        with open("setting.ini", "w", encoding="utf-8") as f:
            config.write(f)
    else:
        send_log_to_server("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè¨­å®šã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        exit()

def download_file(url, dest_path):
    try:
        r = requests.get(url)
        r.raise_for_status()
        with open(dest_path, 'wb') as f:
            f.write(r.content)
        send_log_to_server(f"Downloaded: {url}")
        return True
    except Exception as e:
        send_log_to_server(f"[ERROR] Download failed: {url}\n{e}")
        return False

def check_and_update():
    import os
    import sys

    # 1. æœ€æ–°INIã‚’å–å¾—
    if not download_file(INI_URL, "latest.ini"):
        print("âŒ INIãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        input("Enterã‚­ãƒ¼ã§çµ‚äº†")
        return

    # 2. ãƒãƒ¼ã‚¸ãƒ§ãƒ³èª­ã¿è¾¼ã¿
    latest = configparser.ConfigParser()
    current = configparser.ConfigParser()
    latest.read("latest.ini", encoding="utf-8")
    current.read(LOCAL_INI_PATH, encoding="utf-8")

    latest_ver = latest.get("USER", "version", fallback="0.0.0")
    current_ver = current.get("USER", "version", fallback="0.0.0")

    # 3. ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¯”è¼ƒ
    if latest_ver != current_ver:
        send_log_to_server(f"[INFO] ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚ã‚Šï¼š{current_ver} â†’ {latest_ver}")

        today_str = datetime.now().strftime("%Y%m%d")
        save_dir = os.path.join(os.getcwd(), today_str)
        os.makedirs(save_dir, exist_ok=True)
        new_exe_path = os.path.join(save_dir, "MyCrawler.exe")

        if download_file(EXE_URL, new_exe_path):
            send_log_to_server(f"[INFO] æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ {save_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"\nğŸ”„ æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆ{latest_ver}ï¼‰ã‚’ {save_dir} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            print(f"â¡ï¸ æ¬¡å›ã¯ç¾åœ¨ã®MyCrawler.exeã§ã¯ãªãã€{today_str}/MyCrawler.exeã‚’ä¸Šæ›¸ãå¾Œã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            input("Enterã‚­ãƒ¼ã§çµ‚äº†")
            sys.exit(1)
        else:
            send_log_to_server("[ERROR] æ–°EXEã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            print("âŒ æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            input("Enterã‚­ãƒ¼ã§çµ‚äº†")
            sys.exit(1)
    else:
        send_log_to_server("[INFO] ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯æœ€æ–°ç‰ˆã§ã™")


# èµ·å‹•æ™‚ã«ãƒã‚§ãƒƒã‚¯
check_and_update()

def get_og_image_from_url(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        tag = soup.find("meta", property="og:image")
        return tag["content"] if tag else None
    except Exception:
        return None

# æŠ½å‡ºé–¢æ•°
def extract_field(patterns, text):
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    return None

def extract_email(text):
    match = re.search(r"[\w\.-]+@[\w\.-]+", text)
    return match.group(0) if match else None


# ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now().strftime("%Y-%m-%d")

# ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã€æ—¥ä»˜ãŒå¤‰ã‚ã£ã¦ã„ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
counter_doc = db["crawl_counter"].find_one({"owner": username})
if not counter_doc or counter_doc.get("date") != today:
    maxCountPerDay = 0
    db["crawl_counter"].update_one(
        {"owner": username},
        {"$set": {"count": 0, "date": today}},
        upsert=True
    )
else:
    maxCountPerDay = counter_doc["count"]

def find_contact_page_by_query(top_url):
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    # ãŠå•ã„åˆã‚ã›ã«é–¢ã™ã‚‹ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
    query = f"site:{top_url} ãŠå•ã„åˆã‚ã›"
    driver.get("https://www.bing.com")

    try:
        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›
        search_box = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.NAME, "q"))
        )
        search_box.clear()
        search_box.send_keys(query)
        search_box.submit()
        time.sleep(3)

        # æ¤œç´¢çµæœã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆä¸Šä½æœ€å¤§10ä»¶ï¼‰
        a_tags = driver.find_elements(By.CSS_SELECTOR, "li.b_algo h2 a")
        for a in a_tags:
            href = a.get_attribute("href")
            if href and top_url in href and any(x in href.lower() for x in [
                "contact", "form", "inquiry", "otoiawase", "ãŠå•ã„åˆã‚ã›", "support", "contactus"
            ]):
                return href

    except Exception as e:
        send_log_to_server(f"âŒ ãŠå•ã„åˆã‚ã›ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    return ""  # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™



# ä¼æ¥­æƒ…å ±åé›†é–¢æ•°
def collect_company_info():
    global maxCountPerDay
    for url_doc in urls_collection.find({"owner": username, "status": "æœªåé›†"}):
        try:
            url_1 = url_doc["url"]
            if not is_allowed_by_robots(url_1):
                send_log_to_server(f"â›” robots.txt ã«ã‚ˆã‚Šã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: {url_1}")
                urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "robotsæ‹’å¦"}})
                continue
            company_name = url_doc.get("pre_company_name", "").strip()
            domain = urlparse(url_1).netloc

            if is_same_company(domain, company_name):
                send_log_to_server(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ä¼æ¥­: {company_name} @ {domain}ï¼‰")
                urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—"}})
                continue

            send_log_to_server(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url_1}")

            driver.get(url_1)
            time.sleep(5)
            current_url = driver.current_url
            topurl = urlparse(current_url)
            topurl = f"{topurl.scheme}://{topurl.netloc}"

            result = {}
            result["url_top"] = topurl
            result["eyecatch_image"] = get_og_image_from_url(topurl)

            text = driver.page_source
            text = re.sub(r'<[^>]+>', '', text)
            body_element = driver.find_element(By.TAG_NAME, "body")
            full_text = body_element.get_attribute("innerText")

            driver.get(topurl)
            top_text = driver.page_source
            text = text.replace("\n", "")  # â† ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ
            
            form_data = {
                "company_name": company_name,

                # å¾“æ¥­å“¡æ•°ï¼ˆç¤¾å“¡æ•°ï¼‰
                "employees": extract_field([
                    r"(?:å¾“æ¥­å“¡|ç¤¾å“¡)[^\d0-9ï¼-ï¼™]{0,20}([0-9ï¼-ï¼™,ã€ç™¾åƒä¸‡]{1,20})(äºº|å)?"
                ], text),

                # è³‡æœ¬é‡‘
                "capital": extract_field([
                    r"(?:è³‡æœ¬é‡‘)[^\d0-9ï¼-ï¼™]{0,20}([0-9ï¼-ï¼™,å„„ä¸‡å††]{1,20})"
                ], text),

                # ä½æ‰€ï¼ˆéƒ½é“åºœçœŒåã§é–‹å§‹ï¼‰
                "address": extract_field([
                    r"((åŒ—æµ·é“|é’æ£®çœŒ|å²©æ‰‹çœŒ|å®®åŸçœŒ|ç§‹ç”°çœŒ|å±±å½¢çœŒ|ç¦å³¶çœŒ|èŒ¨åŸçœŒ|æ ƒæœ¨çœŒ|ç¾¤é¦¬çœŒ|åŸ¼ç‰çœŒ|åƒè‘‰çœŒ|æ±äº¬éƒ½|ç¥å¥ˆå·çœŒ|æ–°æ½ŸçœŒ|å¯Œå±±çœŒ|çŸ³å·çœŒ|ç¦äº•çœŒ|å±±æ¢¨çœŒ|é•·é‡çœŒ|å²é˜œçœŒ|é™å²¡çœŒ|æ„›çŸ¥çœŒ|ä¸‰é‡çœŒ|æ»‹è³€çœŒ|äº¬éƒ½åºœ|å¤§é˜ªåºœ|å…µåº«çœŒ|å¥ˆè‰¯çœŒ|å’Œæ­Œå±±çœŒ|é³¥å–çœŒ|å³¶æ ¹çœŒ|å²¡å±±çœŒ|åºƒå³¶çœŒ|å±±å£çœŒ|å¾³å³¶çœŒ|é¦™å·çœŒ|æ„›åª›çœŒ|é«˜çŸ¥çœŒ|ç¦å²¡çœŒ|ä½è³€çœŒ|é•·å´çœŒ|ç†Šæœ¬çœŒ|å¤§åˆ†çœŒ|å®®å´çœŒ|é¹¿å…å³¶çœŒ|æ²–ç¸„çœŒ)[^ã€ã€‚\n\r0-9ï¼-ï¼™ä¸€-ä¹]+)"
                ], full_text),

                # é›»è©±ç•ªå·
                "tel": extract_field([
                    r"(?:Tel|TEL|é›»è©±ç•ªå·|é›»è©±|tel)[^\d0-9ï¼-ï¼™]{0,20}([0-9ï¼-ï¼™]{2,4}[-â€ï¼â€•ãƒ¼â€•\s]?[0-9ï¼-ï¼™]{2,4}[-â€ï¼â€•ãƒ¼â€•\s]?[0-9ï¼-ï¼™]{3,4})"
                ], text),

                # FAXç•ªå·
                "fax": extract_field([
                    r"(?:Fax|FAX|ãƒ•ã‚¡ãƒƒã‚¯ã‚¹|fax)[^\d0-9ï¼-ï¼™]{0,20}([0-9ï¼-ï¼™]{2,4}[-â€ï¼â€•ãƒ¼â€•\s]?[0-9ï¼-ï¼™]{2,4}[-â€ï¼â€•ãƒ¼â€•\s]?[0-9ï¼-ï¼™]{3,4})"
                ], text),

                # è¨­ç«‹å¹´æœˆ
                "founded": extract_field([
                    r"(?:è¨­ç«‹|å‰µç«‹|å‰µæ¥­)[^0-9ï¼-ï¼™]{0,5}([0-9ï¼-ï¼™]{4}å¹´[0-9ï¼-ï¼™]{1,2}æœˆ?)"
                ], text),

                # ä»£è¡¨è€…å
                "ceo": extract_field([
                    r"(ä»£è¡¨å–ç· å½¹[^\n]{0,20})",
                    r"(CEO[^\n]{0,20})"
                ], text),

                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                "email": extract_email(text),

                # ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨èª¬æ˜ï¼ˆmetaã‚¿ã‚°ï¼‰
                "category_keywords": extract_field([
                    r'<meta[^>]+name=["\']keywords["\'][^>]+content=["\'](.*?)["\']'
                ], top_text),

                "description": extract_field([
                    r'<meta[^>]+name=["\']description["\'][^>]+content=["\'](.*?)["\']'
                ], top_text),

                # URLæƒ…å ±ã¯åˆ¥é€”å–å¾—æ¸ˆã¿ã®ã‚‚ã®ã‚’åˆ©ç”¨
                "url_top": topurl,
                "eyecatch_image": result.get("eyecatch_image"),
                "owner": username
            }

            form_url = find_contact_page_by_query(topurl)
            if form_url:
                form_data["url_form"] = form_url

            forms_collection.insert_one(form_data)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})

            maxCountPerDay += 1
            db["crawl_counter"].update_one(
                {"owner": username},
                {"$set": {"count": maxCountPerDay, "date": today}},
                upsert=True
            )
            send_log_to_server(f"âœ… æƒ…å ±åé›†å®Œäº†: {form_data['company_name']} ({current_url})")

        except Exception as ex:
            send_log_to_server(f"âŒ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {ex}")
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})


# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã§åé›†ã¨æ¤œç´¢ã‚’åˆ‡ã‚Šæ›¿ãˆ
while True:
    if maxCountPerDay >= MAX_TOTAL_URLS_PER_DAY:
        send_log_to_server("âœ… æœ€å¤§URLåé›†æ•°ã«é”ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        break  # ã“ã‚Œã«ã‚ˆã‚Š finally ãƒ–ãƒ­ãƒƒã‚¯ã§ driver.quit() ãŒå‘¼ã°ã‚Œã‚‹

    # Seleniumã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆéheadlessï¼‰
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--start-minimized")  # âœ… æœ€å°åŒ–èµ·å‹•
    
    driver = webdriver.Chrome(options=chrome_options)
    
    # OSã«ã‚ˆã‚ŠåŠ¹ã‹ãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§æ‰‹å‹•ã§ç”»é¢å¤–ã¸
    driver.set_window_position(-10000, 0)
    driver.set_window_size(800, 600)
    
    send_log_to_server("âœ… ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–ã—ç”»é¢å¤–ã¸ç§»å‹•ã—ã¾ã—ãŸ")
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–
    send_log_to_server("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–ã—ã¾ã—ãŸ")
    try:
    
        if urls_collection.find_one({"owner": username, "status": "æœªåé›†"}):
            send_log_to_server("ğŸ” æœªåé›†URLãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ä¼æ¥­æƒ…å ±ã®æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            collect_company_info()
        else:
            send_log_to_server("ğŸ” æœªåé›†URLãŒç„¡ã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            keyword_docs = keywords_collection.find({"owner": username})
            keyword_list = [doc["keyword"] for doc in keyword_docs if "keyword" in doc]
    
            if not keyword_list:
                send_log_to_server("âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Bingæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                break
    
            search_query = " ".join(keyword_list) + " æ¦‚è¦ æƒ…å ± -ä¸€è¦§ -ãƒ©ãƒ³ã‚­ãƒ³ã‚° -ã¾ã¨ã‚ -æ¯”è¼ƒ"
            send_log_to_server(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")
    
            driver.get("https://www.bing.com")
            try:
                send_log_to_server("âŒ› Bingãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã„ã¾ã™...")
                search_box = WebDriverWait(driver, 15).until(
                    EC.element_to_be_clickable((By.NAME, "q"))
                )
                send_log_to_server("âœ… æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ä¸­...")
                search_box.clear()
                search_box.send_keys(search_query)
                search_box.submit()
                time.sleep(5)
    
                collected_urls = set()
                send_log_to_server("ğŸ“¥ æ¤œç´¢çµæœã‹ã‚‰URLã‚’åé›†ã—ã¦ã„ã¾ã™...")
                while len(collected_urls) < MAX_NEW_URLS_PER_OWNER:
                    time.sleep(5)
                    results = driver.find_elements(By.CSS_SELECTOR, "li.b_algo")
                    
                    for result in results:
                        try:
                            a_tag = result.find_element(By.CSS_SELECTOR, "h2 a")
                            href = a_tag.get_attribute("href")
                            company_elem = result.find_element(By.CLASS_NAME, "tptt")
                            company_name = company_elem.text.strip() if company_elem else ""
                
                            if href and not urls_collection.find_one({"url": href}):
                                send_log_to_server(f"âœ… æ–°è¦URLç™ºè¦‹: {href}ï¼ˆä¼æ¥­åå€™è£œ: {company_name}ï¼‰")
                                urls_collection.insert_one({
                                    "url": href,
                                    "owner": username,
                                    "keyword": search_query,
                                    "status": "æœªåé›†",
                                    "pre_company_name": company_name
                                })
                                collected_urls.add(href)
                
                                if len(collected_urls) >= MAX_NEW_URLS_PER_OWNER:
                                    break
                        except Exception as e:
                            send_log_to_server(f"âš ï¸ æ¤œç´¢çµæœå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼:{e}")
    
                    next_btn = driver.find_elements(By.CSS_SELECTOR, "a[title='æ¬¡ã®ãƒšãƒ¼ã‚¸']")
                    if next_btn:
                        send_log_to_server("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¾ã™...")
                        try:
                            driver.execute_script("arguments[0].scrollIntoView(true);", next_btn[0])
                            time.sleep(1)
                            next_btn[0].click()
                        except Exception as click_error:
                            send_log_to_server(f"âš ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ã‚¨ãƒ©ãƒ¼:{click_error}")
                            break
                    else:
                        send_log_to_server("â›” æ¬¡ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¤œç´¢çµ‚äº†ã€‚")
                        break
            except Exception as e:
                send_log_to_server(f"âŒ Bingæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼:{e}")
            # Bingæ¤œç´¢å¾Œã«å†åº¦åé›†ãƒ•ã‚§ãƒ¼ã‚ºã«æˆ»ã‚‹ãŸã‚continue
            continue
    except Exception as e:
        send_log_to_server(f"ğŸ”¥ å‡¦ç†ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")

    finally:
        driver.quit()  # âœ… ã“ã‚Œã§ã‚¾ãƒ³ãƒ“Chromeé€€æ²»ï¼
        send_log_to_server("ğŸ§¼ Chromeãƒ‰ãƒ©ã‚¤ãƒã‚’çµ‚äº†ã—ã¾ã—ãŸ")