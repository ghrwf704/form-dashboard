#MyCrawler.py
import configparser
import re
import requests
from urllib.parse import urlparse, urljoin
from pymongo import MongoClient
import certifi
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from datetime import datetime
from tkinter import Tk, simpledialog
import os
from urllib.robotparser import RobotFileParser

# .iniã¿è¾¼ã¿
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")

# ä¼æ¥­åã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã«è¨˜éŒ²ã—ã¦ã‚¹ã‚­ãƒƒãƒ—åˆ¤æ–­
processed_domains = {}

def is_same_company(domain, company_name):
    if not company_name:
        return False  # ç©ºæ¬„ä¼æ¥­åã¯å¸¸ã«é€šã™
    if domain not in processed_domains:
        processed_domains[domain] = set()
    if company_name in processed_domains[domain]:
        return True
    processed_domains[domain].add(company_name)
    return False

def is_allowed_by_robots(url, user_agent='*'):
    try:
        robots_url = urljoin(url, "/robots.txt")
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception as e:
        send_log_to_server(f"âš ï¸ robots.txt ã®ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return True  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è¨±å¯æ‰±ã„

def send_log_to_server(message):
    import configparser
    config = configparser.ConfigParser()
    config.read("setting.ini", encoding="utf-8")
    user = config["USER"].get("id", "unknown")

    print(message)
    try:
        res = requests.get("https://form-dashboard.onrender.com/log", params={"msg": message, "user": user}, timeout=5)
        if res.status_code == 200:
            print("[SERVER] ãƒ­ã‚°é€ä¿¡æˆåŠŸ")
    except Exception as e:
        print(f"[ERROR] ãƒ­ã‚°é€ä¿¡å¤±æ•—: {e}")

# USERã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¢ºèª
if "USER" not in config:
    config["USER"] = {}

# passãƒã‚§ãƒƒã‚¯
if not config["USER"].get("pass"):
    # GUIã§å…¥åŠ›ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰Tkç„¡åŠ¹åŒ–ï¼‰
    root = Tk()
    root.withdraw()
    pw = simpledialog.askstring("åˆå›ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¨­å®š", "ãƒ­ã‚°ã‚¤ãƒ³ç”¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªå‹•çš„ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼‰")
    root.destroy()

    if pw:
        config["USER"]["pass"] = pw
        with open("setting.ini", "w", encoding="utf-8") as f:
            config.write(f)
    else:
        send_log_to_server("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè¨­å®šã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        exit()

INI_URL = "https://form-dashboard.onrender.com/version/latest_setting.ini"  # â† å®Ÿéš›ã®URLã«å¤‰æ›´ã—ã¦ãã ã•ã„
EXE_URL = "https://form-dashboard.onrender.com/downloads/MyCrawler.exe"
LOCAL_INI_PATH = "setting.ini"
EXE_PATH = "MyCrawler.exe"


def download_file(url, dest_path):
    try:
        r = requests.get(url)
        r.raise_for_status()
        with open(dest_path, 'wb') as f:
            f.write(r.content)
        send_log_to_server(f"Downloaded: {url}")
        return True
    except Exception as e:
        send_log_to_server(f"[ERROR] Download failed: {url}\n{e}")
        return False

def check_and_update():
    # 1. æœ€æ–°INIã‚’å–å¾—
    if not download_file(INI_URL, "latest.ini"):
        return  # é€šä¿¡å¤±æ•—ãªã©ã§çµ‚äº†

    # 2. ãƒãƒ¼ã‚¸ãƒ§ãƒ³èª­ã¿è¾¼ã¿
    latest = configparser.ConfigParser()
    current = configparser.ConfigParser()
    latest.read("latest.ini")
    current.read(LOCAL_INI_PATH)

    latest_ver = latest.get("USER", "version", fallback="0.0.0")
    current_ver = current.get("USER", "version", fallback="0.0.0")

    # 3. æ¯”è¼ƒ
    if latest_ver != current_ver:
        send_log_to_server(f"[INFO] ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚ã‚Šï¼š{current_ver} â†’ {latest_ver}")
        # exeãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if download_file(EXE_URL, EXE_PATH):
            # iniæ›´æ–°
            current.set("USER", "version", latest_ver)
            with open(LOCAL_INI_PATH, 'w') as f:
                current.write(f)
            send_log_to_server("[INFO] EXEã¨INIã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    else:
        send_log_to_server("[INFO] ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯æœ€æ–°ç‰ˆã§ã™")

# èµ·å‹•æ™‚ã«ãƒã‚§ãƒƒã‚¯
check_and_update()

def get_og_image_from_url(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        tag = soup.find("meta", property="og:image")
        return tag["content"] if tag else None
    except Exception:
        return None

# å®šæ•°
MAX_NEW_URLS_PER_OWNER = 10
MAX_TOTAL_URLS_PER_DAY = 100
maxCountPerDay = 0

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")
username = config["USER"]["id"]

# MongoDBæ¥ç¶š
MONGO_URI = os.environ.get("MONGO_URI")
client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())
db = client["form_database"]
forms_collection = db["forms"]
keywords_collection = db["keywords"]
urls_collection = db["urls"]
# æŠ½å‡ºé–¢æ•°
def extract_field(patterns, text):
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    return None

def extract_email(text):
    match = re.search(r"[\w\.-]+@[\w\.-]+", text)
    return match.group(0) if match else None


# ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now().strftime("%Y-%m-%d")

# ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã€æ—¥ä»˜ãŒå¤‰ã‚ã£ã¦ã„ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
counter_doc = db["crawl_counter"].find_one({"owner": username})
if not counter_doc or counter_doc.get("date") != today:
    maxCountPerDay = 0
    db["crawl_counter"].update_one(
        {"owner": username},
        {"$set": {"count": 0, "date": today}},
        upsert=True
    )
else:
    maxCountPerDay = counter_doc["count"]

def find_contact_page_by_query(top_url):
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    # ãŠå•ã„åˆã‚ã›ã«é–¢ã™ã‚‹ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
    query = f"site:{top_url} ãŠå•ã„åˆã‚ã›"
    driver.get("https://www.bing.com")

    try:
        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›
        search_box = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.NAME, "q"))
        )
        search_box.clear()
        search_box.send_keys(query)
        search_box.submit()
        time.sleep(3)

        # æ¤œç´¢çµæœã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆä¸Šä½æœ€å¤§10ä»¶ï¼‰
        a_tags = driver.find_elements(By.CSS_SELECTOR, "li.b_algo h2 a")
        for a in a_tags:
            href = a.get_attribute("href")
            if href and top_url in href and any(x in href.lower() for x in [
                "contact", "form", "inquiry", "otoiawase", "ãŠå•ã„åˆã‚ã›", "support", "contactus"
            ]):
                return href

    except Exception as e:
        send_log_to_server(f"âŒ ãŠå•ã„åˆã‚ã›ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    return ""  # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™



# ä¼æ¥­æƒ…å ±åé›†é–¢æ•°
def collect_company_info():
    global maxCountPerDay
    for url_doc in urls_collection.find({"owner": username, "status": "æœªåé›†"}):
        try:
            url_1 = url_doc["url"]
            if not is_allowed_by_robots(url_1):
                send_log_to_server(f"â›” robots.txt ã«ã‚ˆã‚Šã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: {url_1}")
                urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "robotsæ‹’å¦"}})
                continue
            company_name = url_doc.get("pre_company_name", "").strip()
            domain = urlparse(url_1).netloc

            if is_same_company(domain, company_name):
                send_log_to_server(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ä¼æ¥­: {company_name} @ {domain}ï¼‰")
                urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—"}})
                continue

            send_log_to_server(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url_1}")

            driver.get(url_1)
            time.sleep(5)
            current_url = driver.current_url
            topurl = urlparse(current_url)
            topurl = f"{topurl.scheme}://{topurl.netloc}"

            result = {}
            result["url_top"] = topurl
            result["eyecatch_image"] = get_og_image_from_url(topurl)

            text = driver.page_source
            body_element = driver.find_element(By.TAG_NAME, "body")
            full_text = body_element.get_attribute("innerText")

            driver.get(topurl)
            top_text = driver.page_source

            form_data = {
                "company_name": company_name,
                "employees": extract_field([
                    r"å¾“æ¥­å“¡æ•°[:ï¼š\s]*([0-9,]+äºº?)", 
                    r"ç¤¾å“¡æ•°[:ï¼š\s]*([0-9,]+äºº?)"
                ], full_text),
                "capital": extract_field([
                    r"è³‡æœ¬é‡‘[:ï¼š\s]*([0-9,å„„å††ä¸‡å††]+)"
                ], full_text),
                "address": extract_field([
                    r"((åŒ—æµ·é“|é’æ£®çœŒ|å²©æ‰‹çœŒ|å®®åŸçœŒ|ç§‹ç”°çœŒ|å±±å½¢çœŒ|ç¦å³¶çœŒ|èŒ¨åŸçœŒ|æ ƒæœ¨çœŒ|ç¾¤é¦¬çœŒ|åŸ¼ç‰çœŒ|åƒè‘‰çœŒ|æ±äº¬éƒ½|ç¥å¥ˆå·çœŒ|æ–°æ½ŸçœŒ|å¯Œå±±çœŒ|çŸ³å·çœŒ|ç¦äº•çœŒ|å±±æ¢¨çœŒ|é•·é‡çœŒ|å²é˜œçœŒ|é™å²¡çœŒ|æ„›çŸ¥çœŒ|ä¸‰é‡çœŒ|æ»‹è³€çœŒ|äº¬éƒ½åºœ|å¤§é˜ªåºœ|å…µåº«çœŒ|å¥ˆè‰¯çœŒ|å’Œæ­Œå±±çœŒ|é³¥å–çœŒ|å³¶æ ¹çœŒ|å²¡å±±çœŒ|åºƒå³¶çœŒ|å±±å£çœŒ|å¾³å³¶çœŒ|é¦™å·çœŒ|æ„›åª›çœŒ|é«˜çŸ¥çœŒ|ç¦å²¡çœŒ|ä½è³€çœŒ|é•·å´çœŒ|ç†Šæœ¬çœŒ|å¤§åˆ†çœŒ|å®®å´çœŒ|é¹¿å…å³¶çœŒ|æ²–ç¸„çœŒ)[^ã€ã€‚ãƒ»1-9ï¼‘-ï¼™ä¸€-ä¹]+)"
                ], full_text),
                "tel": extract_field([
                    r"(?:Tel|TEL|é›»è©±ç•ªå·|é›»è©±|tel)[^\d]*([0-9ï¼-ï¼™\-\s]{10,15})",
                    r"(\d{2,4}[-â€ï¼â€•\s]?\d{2,4}[-â€ï¼â€•\s]?\d{3,4})"
                ], full_text),
                "fax": extract_field([
                    r"(?:FAX|Fax|ãƒ•ã‚¡ãƒƒã‚¯ã‚¹|fax)[^\d]*([0-9ï¼-ï¼™\-\s]{10,15})"
                ], full_text),
                "founded": extract_field([
                    r"(?:è¨­ç«‹|å‰µç«‹|å‰µæ¥­)[:ï¼š\s]*(\d{4}å¹´\d{1,2}æœˆ?)"
                ], full_text),
                "ceo": extract_field([
                    r"(ä»£è¡¨å–ç· å½¹[^\n]{0,20})", 
                    r"(CEO[^\n]{0,20})"
                ], full_text),
                "email": extract_email(text),
                "category_keywords": extract_field([
                    r'<meta name="keywords" content="(.*?)"'
                ], top_text),
                "description": extract_field([
                    r'<meta name="description" content="(.*?)"'
                ], top_text),
                "url_top": topurl,
                "eyecatch_image": result.get("eyecatch_image"),
                "owner": username
            }

            form_url = find_contact_page_by_query(topurl)
            if form_url:
                form_data["url_form"] = form_url

            forms_collection.insert_one(form_data)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})

            maxCountPerDay += 1
            db["crawl_counter"].update_one(
                {"owner": username},
                {"$set": {"count": maxCountPerDay, "date": today}},
                upsert=True
            )
            send_log_to_server(f"âœ… æƒ…å ±åé›†å®Œäº†: {form_data['company_name']} ({current_url})")

        except Exception as ex:
            send_log_to_server(f"âŒ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {ex}")
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})


# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã§åé›†ã¨æ¤œç´¢ã‚’åˆ‡ã‚Šæ›¿ãˆ
while True:
    # Seleniumã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆéheadlessï¼‰
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)
    driver.minimize_window()
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–
    send_log_to_server("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–ã—ã¾ã—ãŸ")
    if maxCountPerDay >= MAX_TOTAL_URLS_PER_DAY:
        send_log_to_server("âœ… æœ€å¤§URLåé›†æ•°ã«é”ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        break

    if urls_collection.find_one({"owner": username, "status": "æœªåé›†"}):
        send_log_to_server("ğŸ” æœªåé›†URLãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ä¼æ¥­æƒ…å ±ã®æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        collect_company_info()
    else:
        send_log_to_server("ğŸ” æœªåé›†URLãŒç„¡ã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        keyword_docs = keywords_collection.find({"owner": username})
        keyword_list = [doc["keyword"] for doc in keyword_docs if "keyword" in doc]

        if not keyword_list:
            send_log_to_server("âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Bingæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            break

        search_query = " ".join(keyword_list) + " æ¦‚è¦ æƒ…å ± -ä¸€è¦§ -ãƒ©ãƒ³ã‚­ãƒ³ã‚° -ã¾ã¨ã‚ -æ¯”è¼ƒ"
        send_log_to_server(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")

        driver.get("https://www.bing.com")
        try:
            send_log_to_server("âŒ› Bingãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã„ã¾ã™...")
            search_box = WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.NAME, "q"))
            )
            send_log_to_server("âœ… æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ä¸­...")
            search_box.clear()
            search_box.send_keys(search_query)
            search_box.submit()
            time.sleep(5)

            collected_urls = set()
            send_log_to_server("ğŸ“¥ æ¤œç´¢çµæœã‹ã‚‰URLã‚’åé›†ã—ã¦ã„ã¾ã™...")
            while len(collected_urls) < MAX_NEW_URLS_PER_OWNER:
                time.sleep(5)
                results = driver.find_elements(By.CSS_SELECTOR, "li.b_algo")
                
                for result in results:
                    try:
                        a_tag = result.find_element(By.CSS_SELECTOR, "h2 a")
                        href = a_tag.get_attribute("href")
                        company_elem = result.find_element(By.CLASS_NAME, "tptt")
                        company_name = company_elem.text.strip() if company_elem else ""
            
                        if href and not urls_collection.find_one({"url": href}):
                            send_log_to_server(f"âœ… æ–°è¦URLç™ºè¦‹: {href}ï¼ˆä¼æ¥­åå€™è£œ: {company_name}ï¼‰")
                            urls_collection.insert_one({
                                "url": href,
                                "owner": username,
                                "keyword": search_query,
                                "status": "æœªåé›†",
                                "pre_company_name": company_name
                            })
                            collected_urls.add(href)
            
                            if len(collected_urls) >= MAX_NEW_URLS_PER_OWNER:
                                break
                    except Exception as e:
                        send_log_to_server(f"âš ï¸ æ¤œç´¢çµæœå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼:{e}")

                next_btn = driver.find_elements(By.CSS_SELECTOR, "a[title='æ¬¡ã®ãƒšãƒ¼ã‚¸']")
                if next_btn:
                    send_log_to_server("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¾ã™...")
                    try:
                        driver.execute_script("arguments[0].scrollIntoView(true);", next_btn[0])
                        time.sleep(1)
                        next_btn[0].click()
                    except Exception as click_error:
                        send_log_to_server(f"âš ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ã‚¨ãƒ©ãƒ¼:{click_error}")
                        break
                else:
                    send_log_to_server("â›” æ¬¡ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¤œç´¢çµ‚äº†ã€‚")
                    break
        except Exception as e:
            send_log_to_server(f"âŒ Bingæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼:{e}")
        # Bingæ¤œç´¢å¾Œã«å†åº¦åé›†ãƒ•ã‚§ãƒ¼ã‚ºã«æˆ»ã‚‹ãŸã‚continue
        continue
