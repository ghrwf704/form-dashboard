#run_crawler_flow.py
import configparser
import re
import requests
from urllib.parse import urlparse
from pymongo import MongoClient
import certifi
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from datetime import datetime

def get_og_image_from_url(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        tag = soup.find("meta", property="og:image")
        return tag["content"] if tag else None
    except Exception:
        return None

# å®šæ•°
MAX_NEW_URLS_PER_OWNER = 10
MAX_TOTAL_URLS_PER_DAY = 100
maxCountPerDay = 0

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")
username = config["auth"]["id"]

# MongoDBæ¥ç¶š
MONGO_URI = "mongodb+srv://ykeikeikie:qMUerl78WgsEEOWA@cluster0.helfbov.mongodb.net/?retryWrites=true&w=majority"
client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())
db = client["form_database"]
urls_collection = db["urls"]
forms_collection = db["forms"]
keywords_collection = db["keywords"]

# æŠ½å‡ºé–¢æ•°
def extract_field(patterns, text):
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    return None

def extract_email(text):
    match = re.search(r"[\w\.-]+@[\w\.-]+", text)
    return match.group(0) if match else None


# ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now().strftime("%Y-%m-%d")

# ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã€æ—¥ä»˜ãŒå¤‰ã‚ã£ã¦ã„ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
counter_doc = db["crawl_counter"].find_one({"owner": username})
if not counter_doc or counter_doc.get("date") != today:
    maxCountPerDay = 0
    db["crawl_counter"].update_one(
        {"owner": username},
        {"$set": {"count": 0, "date": today}},
        upsert=True
    )
else:
    maxCountPerDay = counter_doc["count"]

# ä¼æ¥­æƒ…å ±åé›†é–¢æ•°
def collect_company_info():
    global maxCountPerDay
    for url_doc in urls_collection.find({"owner": username, "status": "æœªåé›†"}):
        try:
            url_1 = url_doc["url"]
            print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url_1}")

            driver.get(url_1)
            time.sleep(5)
            current_url = driver.current_url
            topurl = urlparse(current_url)
            topurl = f"{topurl.scheme}://{topurl.netloc}"
            result = {}  # ç©ºã®è¾æ›¸ã¨ã—ã¦åˆæœŸåŒ–
            result["url_top"] = topurl
            result["eyecatch_image"] = get_og_image_from_url(topurl)
            text = driver.page_source
            body_element = driver.find_element(By.TAG_NAME, "body")
            full_text = body_element.get_attribute("innerText")
            driver.get(topurl)
            top_text = driver.page_source
            form_data = {
                "company_name": url_doc.get("pre_company_name"),
                
                "employees": extract_field([
                    r"å¾“æ¥­å“¡æ•°[:ï¼š\s]*([0-9,]+äºº?)", 
                    r"ç¤¾å“¡æ•°[:ï¼š\s]*([0-9,]+äºº?)"
                ], full_text),
            
                "capital": extract_field([
                    r"è³‡æœ¬é‡‘[:ï¼š\s]*([0-9,å„„å††ä¸‡å††]+)"
                ], full_text),
            
                "address": extract_field([
                    r"((åŒ—æµ·é“|é’æ£®çœŒ|å²©æ‰‹çœŒ|å®®åŸçœŒ|ç§‹ç”°çœŒ|å±±å½¢çœŒ|ç¦å³¶çœŒ|èŒ¨åŸçœŒ|æ ƒæœ¨çœŒ|ç¾¤é¦¬çœŒ|åŸ¼ç‰çœŒ|åƒè‘‰çœŒ|æ±äº¬éƒ½|ç¥å¥ˆå·çœŒ|æ–°æ½ŸçœŒ|å¯Œå±±çœŒ|çŸ³å·çœŒ|ç¦äº•çœŒ|å±±æ¢¨çœŒ|é•·é‡çœŒ|å²é˜œçœŒ|é™å²¡çœŒ|æ„›çŸ¥çœŒ|ä¸‰é‡çœŒ|æ»‹è³€çœŒ|äº¬éƒ½åºœ|å¤§é˜ªåºœ|å…µåº«çœŒ|å¥ˆè‰¯çœŒ|å’Œæ­Œå±±çœŒ|é³¥å–çœŒ|å³¶æ ¹çœŒ|å²¡å±±çœŒ|åºƒå³¶çœŒ|å±±å£çœŒ|å¾³å³¶çœŒ|é¦™å·çœŒ|æ„›åª›çœŒ|é«˜çŸ¥çœŒ|ç¦å²¡çœŒ|ä½è³€çœŒ|é•·å´çœŒ|ç†Šæœ¬çœŒ|å¤§åˆ†çœŒ|å®®å´çœŒ|é¹¿å…å³¶çœŒ|æ²–ç¸„çœŒ)[^ã€ã€‚ãƒ»1-9ï¼‘-ï¼™ä¸€-ä¹]+)"
                ], full_text),
            
                "tel": extract_field([
                    r"(?:Tel|TEL|é›»è©±ç•ªå·|é›»è©±|tel)[^\d]*([0-9ï¼-ï¼™\-\s]{10,15})",
                    r"(\d{2,4}[-â€ï¼â€•\s]?\d{2,4}[-â€ï¼â€•\s]?\d{3,4})"
                ], full_text),
            
                "fax": extract_field([
                    r"(?:FAX|Fax|ãƒ•ã‚¡ãƒƒã‚¯ã‚¹|fax)[^\d]*([0-9ï¼-ï¼™\-\s]{10,15})"
                ], full_text),
            
                "founded": extract_field([
                    r"(?:è¨­ç«‹|å‰µç«‹|å‰µæ¥­)[:ï¼š\s]*(\d{4}å¹´\d{1,2}æœˆ?)"
                ], full_text),
            
                "ceo": extract_field([
                    r"(ä»£è¡¨å–ç· å½¹[^\n]{0,20})", 
                    r"(CEO[^\n]{0,20})"
                ], full_text),
            
                "email": extract_email(text),
            
                "category_keywords": extract_field([
                    r'<meta name="keywords" content="(.*?)"'
                ], top_text),
            
                "description": extract_field([
                    r'<meta name="description" content="(.*?)"'
                ], top_text),
            
                "url_top": topurl,
            
                "url_form": extract_field([
                    r"(https?://[\w/:%#\$&\?\(\)~\.\=\+\-]+(contact|inquiry|form)[^\"'<>]*)"
                ], text),
                "eyecatch_image": result.get("eyecatch_image"),

                "owner": username
            }


            forms_collection.insert_one(form_data)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})
            maxCountPerDay += 1
            db["crawl_counter"].update_one(
                {"owner": username},
                {"$set": {"count": maxCountPerDay, "date": today}},
                upsert=True
            )
            print(f"âœ… æƒ…å ±åé›†å®Œäº†: {form_data['company_name']} ({current_url})")
        except Exception as ex:
            print("âŒ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼:", ex)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})

# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã§åé›†ã¨æ¤œç´¢ã‚’åˆ‡ã‚Šæ›¿ãˆ
while True:
    # Seleniumã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆéheadlessï¼‰
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)
    driver.minimize_window()
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–
    print("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å°åŒ–ã—ã¾ã—ãŸ")
    if maxCountPerDay >= MAX_TOTAL_URLS_PER_DAY:
        print("âœ… æœ€å¤§URLåé›†æ•°ã«é”ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        break

    if urls_collection.find_one({"owner": username, "status": "æœªåé›†"}):
        print("ğŸ” æœªåé›†URLãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ä¼æ¥­æƒ…å ±ã®æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        collect_company_info()
    else:
        print("ğŸ” æœªåé›†URLãŒç„¡ã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        keyword_docs = keywords_collection.find({"owner": username})
        keyword_list = [doc["keyword"] for doc in keyword_docs if "keyword" in doc]

        if not keyword_list:
            print("âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Bingæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            break

        search_query = " ".join(keyword_list) + " æ¦‚è¦ æƒ…å ± -ä¸€è¦§ -ãƒ©ãƒ³ã‚­ãƒ³ã‚° -ã¾ã¨ã‚ -æ¯”è¼ƒ"
        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")

        driver.get("https://www.bing.com")
        try:
            print("âŒ› Bingãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦ã„ã¾ã™...")
            search_box = WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.NAME, "q"))
            )
            print("âœ… æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ä¸­...")
            search_box.clear()
            search_box.send_keys(search_query)
            search_box.submit()
            time.sleep(5)

            collected_urls = set()
            print("ğŸ“¥ æ¤œç´¢çµæœã‹ã‚‰URLã‚’åé›†ã—ã¦ã„ã¾ã™...")
            while len(collected_urls) < MAX_NEW_URLS_PER_OWNER:
                time.sleep(5)
                results = driver.find_elements(By.CSS_SELECTOR, "li.b_algo")
                
                for result in results:
                    try:
                        a_tag = result.find_element(By.CSS_SELECTOR, "h2 a")
                        href = a_tag.get_attribute("href")
                        company_elem = result.find_element(By.CLASS_NAME, "tptt")
                        company_name = company_elem.text.strip() if company_elem else ""
            
                        if href and not urls_collection.find_one({"url": href}):
                            print(f"âœ… æ–°è¦URLç™ºè¦‹: {href}ï¼ˆä¼æ¥­åå€™è£œ: {company_name}ï¼‰")
                            urls_collection.insert_one({
                                "url": href,
                                "owner": username,
                                "keyword": search_query,
                                "status": "æœªåé›†",
                                "pre_company_name": company_name
                            })
                            collected_urls.add(href)
            
                            if len(collected_urls) >= MAX_NEW_URLS_PER_OWNER:
                                break
                    except Exception as e:
                        print("âš ï¸ æ¤œç´¢çµæœå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼:", e)

                next_btn = driver.find_elements(By.CSS_SELECTOR, "a[title='æ¬¡ã®ãƒšãƒ¼ã‚¸']")
                if next_btn:
                    print("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¾ã™...")
                    try:
                        driver.execute_script("arguments[0].scrollIntoView(true);", next_btn[0])
                        time.sleep(1)
                        next_btn[0].click()
                    except Exception as click_error:
                        print("âš ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ã‚¨ãƒ©ãƒ¼:", click_error)
                        break
                else:
                    print("â›” æ¬¡ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¤œç´¢çµ‚äº†ã€‚")
                    break
        except Exception as e:
            print("âŒ Bingæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼:", e)
        # Bingæ¤œç´¢å¾Œã«å†åº¦åé›†ãƒ•ã‚§ãƒ¼ã‚ºã«æˆ»ã‚‹ãŸã‚continue
        continue
