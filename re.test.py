import configparser
import re
import requests
from urllib.parse import urlparse
from pymongo import MongoClient
import certifi
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from datetime import datetime

# å®šæ•°
MAX_NEW_URLS_PER_OWNER = 10
MAX_TOTAL_URLS_PER_DAY = 100
maxCountPerDay = 0

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
config = configparser.ConfigParser()
config.read("setting.ini", encoding="utf-8")
username = config["auth"]["id"]

# MongoDBæ¥ç¶š
MONGO_URI = "mongodb+srv://ykeikeikie:qMUerl78WgsEEOWA@cluster0.helfbov.mongodb.net/?retryWrites=true&w=majority"
client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())
db = client["form_database"]
urls_collection = db["urls"]
forms_collection = db["forms"]
keywords_collection = db["keywords"]

# ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
counter_deleted = db["crawl_counter"].delete_many({"owner": username})
print(f"crawl_counter å‰Šé™¤æ•°: {counter_deleted.deleted_count}")

# æŠ½å‡ºé–¢æ•°
def extract_field(patterns, text):
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    return None

def extract_email(text):
    match = re.search(r"[\w\.-]+@[\w\.-]+", text)
    return match.group(0) if match else None

def lookup_company_by_phone(phone_number):
    try:
        url = f"https://www.jpnumber.com/searchnumber.do?number={phone_number}"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        name_tag = soup.select_one(".searchResults td b")
        addr_tag = soup.select_one(".searchResults td")

        name = name_tag.text.strip() if name_tag else None
        address = addr_tag.get_text(strip=True) if addr_tag else None

        return {
            "company_name_from_phone": name,
            "address_from_phone": address
        }
    except Exception as e:
        print(f"âŒ é›»è©±ç•ªå·é€†å¼•ãå¤±æ•—: {e}")
        return {}

# Seleniumã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆéheadlessï¼‰
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=chrome_options)

# ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now().strftime("%Y-%m-%d")

# ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã€æ—¥ä»˜ãŒå¤‰ã‚ã£ã¦ã„ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
counter_doc = db["crawl_counter"].find_one({"owner": username})
if not counter_doc or counter_doc.get("date") != today:
    maxCountPerDay = 0
    db["crawl_counter"].update_one(
        {"owner": username},
        {"$set": {"count": 0, "date": today}},
        upsert=True
    )
else:
    maxCountPerDay = counter_doc["count"]

# ä¼æ¥­æƒ…å ±åé›†é–¢æ•°
def collect_company_info():
    global maxCountPerDay
    for url_doc in urls_collection.find({"owner": username, "status": "æœªåé›†"}):
        try:
            url_1 = url_doc["url"]
            print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url_1}")

            driver.get(url_1)
            time.sleep(5)
            current_url = driver.current_url
            text = driver.page_source
            print("ğŸ“„ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸã€‚æƒ…å ±ã‚’æŠ½å‡ºä¸­...")

            with open("last_page_source.html", "w", encoding="utf-8") as f:
                f.write(text)
            print("ğŸ“ ãƒšãƒ¼ã‚¸HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆlast_page_source.htmlï¼‰")

            tel = extract_field([
                r"TEL[:ï¼š]?\s*(\d{2,4}[-â€ï¼â€•â€•\s]?\d{2,4}[-â€ï¼â€•â€•\s]?\d{3,4})",
                r"é›»è©±[:ï¼š]?\s*(\d{2,4}[-â€ï¼â€•â€•\s]?\d{2,4}[-â€ï¼â€•â€•\s]?\d{3,4})"
            ], text)

            fax = extract_field([
                r"FAX[:ï¼š]?\s*(\d{2,4}[-â€ï¼â€•â€•\s]?\d{2,4}[-â€ï¼â€•â€•\s]?\d{3,4})",
                r"ãƒ•ã‚¡ãƒƒã‚¯ã‚¹[:ï¼š]?\s*(\d{2,4}[-â€ï¼â€•â€•\s]?\d{2,4}[-â€ï¼â€•â€•\s]?\d{3,4})"
            ], text)

            address = extract_field([
                r"ã€’?\d{3}-\d{4}.+?[éƒ½é“åºœçœŒ].+?[å¸‚åŒºç”ºæ‘].+?",
                r"ä½æ‰€[:ï¼š]?\s*(.+?[å¸‚åŒºç”ºæ‘].+)"
            ], text)

            company_name = extract_field([
                r"(?:(?:ä¼šç¤¾å|ç¤¾å|å•†å·|æ³•äººå|åç§°|åŒ»ç™‚æ³•äºº|åˆåŒä¼šç¤¾|ã‚¯ãƒªãƒ‹ãƒƒã‚¯)[ï¼š:\s]*)?([\u4E00-\u9FFF\w\s\(\)\-\u30A0-\u30FF]{3,})"
            ], text) or url_doc.get("pre_company_name")

            if not company_name and tel:
                info = lookup_company_by_phone(tel)
                company_name = info.get("company_name_from_phone")
                address = address or info.get("address_from_phone")

            form_data = {
                "company_name": company_name,
                "employees": extract_field([r"å¾“æ¥­å“¡æ•°[:ï¼š]?\s*([0-9,]+äºº?)", r"ç¤¾å“¡æ•°[:ï¼š]?\s*([0-9,]+äºº?)"], text),
                "capital": extract_field([r"è³‡æœ¬é‡‘[:ï¼š]?\s*([0-9,å„„å††ä¸‡å††]+)"], text),
                "address": address,
                "tel": tel,
                "fax": fax,
                "founded": extract_field([r"(?:è¨­ç«‹|å‰µç«‹|å‰µæ¥­)[:ï¼š]?\s*(\d{4}å¹´\d{1,2}æœˆ?)"], text),
                "ceo": extract_field([r"(ä»£è¡¨å–ç· å½¹[^\n]{0,20})", r"(CEO[^\n]{0,20})"], text),
                "email": extract_email(text),
                "category_keywords": extract_field([r'<meta name="keywords" content="(.*?)"'], text),
                "description": extract_field([r'<meta name="description" content="(.*?)"'], text),
                "url_top": current_url,
                "url_form": extract_field([r"(https?://[\w/:%#\$&\?\(\)~\.\=\+\-]+(contact|inquiry|form)[^\"'<>]*)"], text),
                "owner": username
            }

            print("ğŸ“ æŠ½å‡ºçµæœ:", form_data)
            forms_collection.insert_one(form_data)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})
            maxCountPerDay += 1
            db["crawl_counter"].update_one(
                {"owner": username},
                {"$set": {"count": maxCountPerDay, "date": today}},
                upsert=True
            )
            print(f"âœ… æƒ…å ±åé›†å®Œäº†: {form_data['company_name']} ({current_url})")
        except Exception as ex:
            print("âŒ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼:", ex)
            urls_collection.update_one({"_id": url_doc["_id"]}, {"$set": {"status": "åé›†æ¸ˆ"}})
